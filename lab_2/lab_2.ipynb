{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful models\n",
    "\n",
    "A “stateful model” is a model that implicitly preserves data between two consecutive inference calls. The tensors saved from one run are kept in an internal memory buffer called a “state” or a “variable” and may be passed to the next run, while never being exposed as model output. In contrast, for a “stateless” model to pass data between runs, all produced data is returned as output and needs to be handled by the application itself for reuse at the next execution.\n",
    "\n",
    "![avatar](stateless%20vs%20stateful.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export a stateless INT8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py --model_id THUDM/chatglm3-6b --disable-stateful -c INT8 --output_dir chatglm3-6b-ov-int8-stateless -p FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export a stateless INT4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py --model_id THUDM/chatglm3-6b --disable-stateful -c INT4_SYM --ratio 0.8 --group_size 128 --output_dir chatglm3-6b-ov-int4-stateless -p FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export a stateful INT4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py --model_id THUDM/chatglm3-6b --stateful -c INT4_SYM --ratio 0.8 --group_size 128 --output_dir chatglm3-6b-ov-int4-stateful -p FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramters:\n",
    "* `--model_id` - model_id for downloading from huggngface_hub (https://huggingface.co/models) or path with directory where pytorch model located.\n",
    "* `--output_dir` - output directory for saving OpenVINO model\n",
    "* `--precision` - (optional, default FP32), precision for model conversion FP32 or FP16\n",
    "* `--save_orig` - flag for saving original pytorch model, model will be located in `<output_dir>/pytorch` subdirectory.\n",
    "* `--compress_weights` - The weight compression option, INT8 - INT8 weights, 4BIT_DEFAULT - for 4-bit weights compression with predefined configuration, INT4_SYM - for INT4 compressed weights with symmetric quantization, INT4_ASYM - for INT4 compressed weights with assymetric quantization. You can specify multiple backends separated by a space.\n",
    "* `--compress_weights_backends` - (optional, default openvino) backends for weights compression, this option has an effect only with `--compress_weights`. You can specify multiple backends separated by a space.\n",
    "* `--ratio` - Compression ratio between primary and backup precision, e.g. INT4/INT8.\n",
    "* `--group_size` - Size of the group of weights that share the same quantization parameters\n",
    "\n",
    "\n",
    "If you have difficulty accessing `huggingface`, you can try to use `mirror-hf` to download:\n",
    "\n",
    "  * Linux\n",
    "\n",
    "    ```\n",
    "    export HF_ENDPOINT=https://hf-mirror.com\n",
    "    ```\n",
    "   \n",
    "  * Windows Powershell\n",
    "\n",
    "    ```\n",
    "    $env:HF_ENDPOINT = \"https://hf-mirror.com\"\n",
    "    ```\n",
    "  \n",
    "  * Download model\n",
    "  \n",
    "    ```\n",
    "    huggingface-cli download --resume-download --local-dir-use-symlinks False THUDM/chatglm3-6b --local-dir {your_path}/chatglm3-6b\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why stateful models ?\n",
    "\n",
    "Text generation is a good usage example of stateful models, as it requires multiple inference calls to output a complete sentence, each run producing a single output token. Information from one run is passed to the next inference as a context, which may be handled by a stateful model natively. Potential benefits for this, as well as other scenarios, may be:\n",
    "\n",
    "model execution speedup - data in states is stored in the optimized form for OpenVINO plugins, which helps to execute the model more efficiently. Importantly, requesting data from the state too often may reduce the expected performance gains or even lead to losses. Use the state mechanism only if the state data is not accessed very frequently.\n",
    "\n",
    "user code simplification - states can replace code-based solutions for such scenarios as giving initializing values for the first inference call or copying data from model outputs to inputs. With states, OpenVINO will manage these cases internally, additionally removing the potential for additional overhead due to data representation conversion.\n",
    "\n",
    "data processing - some use cases require processing of data sequences. When such a sequence is of known length and short enough, you can process it with RNN-like models that contain a cycle inside. When the length is not known, as in the case of online speech recognition or time series forecasting, you can divide the data in small portions and process it step-by-step, which requires addressing the dependency between data portions. States fulfil this purpose well: models save some data between inference runs, when one dependent sequence is over, the state may be reset to the initial value and a new sequence can be started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark for stateless INT8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark.py -m \"chatglm3-6b-ov-int4-stateful\\pytorch\\dldt\\compressed_weights\\OV_FP16-INT8\" -n 5 -d gpu -p \"若我有一亿美元，在人工智能盛行的今天，我怎样投资才能收益最大化？\"  -ic 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark for stateless INT4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python benchmark.py -m \"chatglm3-6b-ov-int4-stateless\\pytorch\\dldt\\compressed_weights\\OV_FP16-INT4_SYM\" -n 5 -d gpu -p \"若我有一亿美元，在人工智能盛行的今天，我怎样投资才能收益最大化？\"  -ic 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmark with stateful INT4  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark.py -m \"chatglm3-6b-ov-int4-stateful\\pytorch\\dldt\\compressed_weights\\OV_FP16-INT4_SYM\" -n 5 -d gpu -p \"若我有一亿美元，在人工智能盛行的今天，我怎样投资才能收益最大化？\"  -ic 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramters:\n",
    "* `-m` - model path\n",
    "* `-d` - inference device (default=cpu)\n",
    "* `-r` - report csv\n",
    "* `-f` - framework (default=ov)\n",
    "* `-p` - interactive prompt text\n",
    "* `-pf` - path of JSONL file including interactive prompts\n",
    "* `-n` - number of benchmarking iterations, if the value greater 0, will exclude the first iteration. (default=0)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/4547501/138267961-41d754e7-59db-49f6-b700-63c3a636fad7.gif",
   "tags": {
    "categories": [
     "Live Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Pose Estimation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
